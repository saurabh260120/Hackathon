{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "problem-context",
   "metadata": {},
   "source": [
    "# Automated Regulatory Data Profiling System\n",
    "## End-to-End Compliance Pipeline with AI/ML Components"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "architecture-overview",
   "metadata": {},
   "source": [
    "### System Architecture:\n",
    "1. PDF Parser & Rule Extractor\n",
    "2. Validation Code Generator\n",
    "3. Anomaly Detection Engine\n",
    "4. Risk Scoring System\n",
    "5. Remediation Advisor\n",
    "6. Interactive Auditor UI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "enhanced-pdf-parser",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from typing import List, Dict\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def enhanced_pdf_parser(pdf_path: str) -> List[Dict]:\n",
    "    \"\"\"Improved PDF text extraction with preprocessing\"\"\"\n",
    "    raw_text = extract_text_from_pdf(pdf_path)\n",
    "    \n",
    "    # Clean text artifacts\n",
    "    cleaned_text = re.sub(r'(\\w)\\s+(\\w)', r'\\1\\2', raw_text)  # Fix mid-word spaces\n",
    "    cleaned_text = re.sub(r'\\n\\s+\\n', '\\n', cleaned_text)  # Remove empty lines\n",
    "    \n",
    "    # Enhanced LLM prompt with formatting examples\n",
    "    prompt = f\"\"\"Extract fields from this regulatory document. \n",
    "    Handle variations in spacing and line breaks. Example conversion:\n",
    "    'Obli gor Name' â†’ 'ObligorName'\n",
    "    Document Content:\\n{cleaned_text}\\n\"\"\"\n",
    "    \n",
    "    return process_with_llm(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dynamic-validation-generator",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_validation_functions(rules: List[Dict]) -> Dict:\n",
    "    \"\"\"Create reusable validation functions with shared constraints\"\"\"\n",
    "    validation_map = {}\n",
    "    \n",
    "    # Shared constraint checker\n",
    "    invalid_chars = ['\\r', '\\n', ','] + [chr(i) for i in range(0,32)]\n",
    "    \n",
    "    for rule in rules:\n",
    "        tech_name = rule['technical_name']\n",
    "        \n",
    "        if \"Must not contain\" in rule.get('constraints', ''):\n",
    "            code = f\"\"\"def validate_{tech_name}(value):\n",
    "                return not any(char in value for char in {invalid_chars})\"\"\"\n",
    "        elif \"country code\" in rule.get('description', ''):\n",
    "            code = \"\"\"def validate_{}(value):\n",
    "                return len(value) == 2 and value.isalpha()\"\"\".format(tech_name)\n",
    "        else:\n",
    "            code = f\"def validate_{tech_name}(value): return True\"\n",
    "        \n",
    "        validation_map[tech_name] = code\n",
    "    \n",
    "    return validation_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "anomaly-detection",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RiskAnalyzer:\n",
    "    def __init__(self):\n",
    "        self.scaler = StandardScaler()\n",
    "        self.cluster_model = DBSCAN(eps=0.5, min_samples=5)\n",
    "    \n",
    "    def calculate_risk_scores(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Calculate anomaly scores using unsupervised learning\"\"\"\n",
    "        numeric_cols = df.select_dtypes(include='number').columns\n",
    "        scaled_data = self.scaler.fit_transform(df[numeric_cols])\n",
    "        \n",
    "        # Cluster analysis\n",
    "        clusters = self.cluster_model.fit_predict(scaled_data)\n",
    "        df['cluster_group'] = clusters\n",
    "        \n",
    "        # Distance from cluster centroids as risk score\n",
    "        df['risk_score'] = df.apply(lambda row: \n",
    "            np.linalg.norm(scaled_data[row.name] - \n",
    "            scaled_data[clusters == row['cluster_group']].mean(axis=0)), axis=1)\n",
    "        \n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "remediation-engine",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_remediation(failed_rules: List[str]) -> str:\n",
    "    \"\"\"AI-powered remediation suggestions\"\"\"\n",
    "    prompt = f\"Generate remediation steps for these validation failures: {failed_rules}\"\n",
    "    return model.generate_content(prompt).text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "interactive-ui",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "\n",
    "def create_auditor_interface():\n",
    "    with gr.Blocks() as ui:\n",
    "        with gr.Row():\n",
    "            pdf_input = gr.File(label=\"Upload Regulatory PDF\")\n",
    "            csv_input = gr.File(label=\"Upload Transaction Data\")\n",
    "        \n",
    "        with gr.Accordion(\"Advanced Settings\"):\n",
    "            risk_threshold = gr.Slider(0, 1, value=0.7, label=\"Risk Threshold\")\n",
    "            \n",
    "        report_output = gr.JSON(label=\"Compliance Report\")\n",
    "        \n",
    "        gr.Interface(\n",
    "            fn=process_files,\n",
    "            inputs=[pdf_input, csv_input, risk_threshold],\n",
    "            outputs=report_output,\n",
    "            live=True\n",
    "        )\n",
    "    \n",
    "    return ui"
   ]
  }
 ],
 "metadata": {}
}